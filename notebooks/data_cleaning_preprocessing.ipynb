{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## This is a code to clean the data for customer feedback clustering and topic modeling\n",
    "### Authors: Mahdi Rasouli, Amir Abdollahi, Christian Bonato"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing the necessary libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path as Path\n",
    "import datetime\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'mebank','um','umm','ummm','hi','hello','hey','heyyyyy','fyi'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/amir.abdollahi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amir.abdollahi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining the functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "def preprocess_text(text, flg_clean=True, flg_tweet=True, flg_stemm=False, flg_lemm=False, lst_stopwords=None):\n",
    "\n",
    "\t## Tweet preprocessor\n",
    "\tif flg_tweet == True:\n",
    "\t\timport preprocessor as p\n",
    "\t\t# remove url, mention,emoji, smily, and numbers (keeping hashtags)\n",
    "\t\tp.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "\t\ttext=p.clean(text)\n",
    "\n",
    "\t## Tweet preprocessor\n",
    "\tif flg_clean == True:\n",
    "\n",
    "\t\t# Remove mentions\n",
    "\t\ttext = re.sub(r'/^(?!.*\\bRT\\b)(?:.+\\s)?@\\w+/i', '', text)\n",
    "\t\ttext = re.sub('@', '', text)\n",
    "\t\t\n",
    "\t\t# Replace Emails\n",
    "\t\ttext = re.sub('\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "\t\t# Remove links\n",
    "\t\ttext = re.sub('http\\S*', '', text)\n",
    "\n",
    "\t\t# clean hashtags (just removing the hashtag)\n",
    "\t\t# #text = re.sub('#\\S*', '', text)\n",
    "\t\ttext = re.sub('#', '', text)\n",
    "\n",
    "\t\t# Remove unacceptable characters/emojis\n",
    "\t\ttext = re.sub('\\S*ü\\S*\\s?', '', text)\n",
    "\t\ttext = re.sub('\\S*ò\\S*\\s?', '', text)\n",
    "\t\ttext = re.sub('\\S*ä\\S*\\s?', '', text)\n",
    "\t\ttext = re.sub('\\S*ô\\S*\\s?', '', text)\n",
    "\n",
    "\n",
    "\t\t# Remove new line characters\n",
    "\t\ttext = re.sub('\\s+', ' ', text)\n",
    "\n",
    "\t\t# convert to lower case\n",
    "\t\ttext=text.lower()\n",
    "\n",
    "\t\t## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "\t\t#text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "\t\t\n",
    "\t\t# Remove distracting single quotes\n",
    "\t\t#text = [re.sub(\"\\'\", \"\", sent) for sent in text]\n",
    "\n",
    "\t\t\t\n",
    "\t## Tokenize (convert from string to list)\n",
    "\tlst_text = text.split()    ## remove Stopwords\n",
    "\n",
    "\tif lst_stopwords is not None:\n",
    "\t\tlst_text = [word for word in lst_text if word not in lst_stopwords]\n",
    "\t\t\n",
    "\t## Stemming (remove -ing, -ly, ...)\n",
    "\tif flg_stemm == True:\n",
    "\t\tps = nltk.stem.porter.PorterStemmer()\n",
    "\t\tlst_text = [ps.stem(word) for word in lst_text]\n",
    "\t\t\n",
    "\t## Lemmatisation (convert the word into root word)\n",
    "\tif flg_lemm == True:\n",
    "\t\tlem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\t\tlst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "\t\t\n",
    "\t## back to string from list\n",
    "\ttext = \" \".join(lst_text)\n",
    "\t\n",
    "\treturn text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading the input data from AWS S3 \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "s3_data_dir=\"s3://ds-rg271/data\"\n",
    "input_data_url = Path.join(s3_data_dir,\"labelled/mebank_tweets_1_year_labelled.csv\")\n",
    "input_data=pd.read_csv(input_data_url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "print(\"Shape of the input data is:\", input_data.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the input data is: (897, 8)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing duplicate records with the same content (tweet) and date \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "print(\"Number of removed duplicate records is:\", len(input_data) - len(input_data.drop_duplicates([\"content\",\"date\"])))\n",
    "input_data.drop_duplicates([\"content\",\"date\"], inplace = True)\n",
    "input_data.reset_index(inplace= True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of removed duplicate records is: 21\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Changing the format of the tweet dates to datetime"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "for i, date_str in enumerate(input_data['date']):\n",
    "\tdate_str=re.sub('\\+00:00', '', date_str)\n",
    "\tinput_data.loc[i,'date']=datetime.datetime.strptime(date_str,'%Y-%m-%d %H:%M:%S')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sorting the data based on date"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "input_data_sorted=input_data.sort_values(by=['date'],ignore_index=True).reset_index(drop=True)\n",
    "print(\"Shape of the sorted data is:\", input_data_sorted.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of the sorted data is: (876, 9)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keeping only complaint 1 and no-complaint 0 records (Removing 0.5 and -1 labels)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "input_data_01=input_data_sorted[input_data_sorted.complaint.isin([0, 1])].reset_index(drop=True)\n",
    "input_data_01.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(842, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ensure all labels are the same by making them lower case and stripping trailing whitespace\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "input_data_01[\"topic\"] = input_data_01[\"topic\"].str.lower().str.strip()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ensure that complaints are integer (0 or 1)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "input_data_01[\"complaint\"] = input_data_01[\"complaint\"].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fixing the missed problem - other"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "input_data_01.loc[input_data_01[\"topic\"] == \"problem - other\", \"topic\"] = \"problem/others\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Uploading the cleaned data to S3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "input_data_01.to_csv(f\"{s3_data_dir}/labelled/mebank_tweets_1_year_cleaned.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing the input text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "input_data_clean=input_data_01.copy()\n",
    "for i in range(0,len(input_data_clean['content'])):\n",
    "\tinput_data_clean.loc[i,'content_clean']=preprocess_text(input_data_clean.loc[i,'content'], flg_clean=True, flg_tweet=True, flg_stemm=False, flg_lemm=False, lst_stopwords=None)\n",
    "# Selecting the relevant features\n",
    "input_data_clean=input_data_clean[['date','content','content_clean','complaint','topic']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_data_clean.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing the empty records in \"content_clean\" "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_data_clean.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "input_data_clean.dropna(subset = [\"content_clean\"], inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_data_clean.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Uploading the preprocessed data to S3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "input_data_clean.to_csv(f\"{s3_data_dir}/preprocessed/mebank_tweets_1_year_preprocessed.csv\", index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "5931c7d52e9fc51453d695c69bd7bc0e29a8609cc3f04d6bd48a56509bd64bdb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}